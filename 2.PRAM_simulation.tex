\section{PRAM Simulation}

Parallel machines that communicate via shared memory
(i.e. PRAM, Parallel Random Access Machine) are the most
commonly used computational model for describing parallel algorithms.
They provide a full abstraction over the memory management assuming
that each processor can access concurrently a shared memory. While being
very comfortable to program those machines are unrealistic from a technological
viewpoint, in fact parallel shared memory access can only be realized at the
cost of a significant delay. A more realistic model that takes distributed
memory into account is constituted by distributed memory machines (DMM) in which
the memory is divided into modules, one per processor. Since DMM's modules can
handle only one request at a time, it emerges a phenomenon
called \textit{memory contention}.
In trying to get a deeper understanding of the performance
effects of memory contention it is useful to investigate the simulation of
 PRAM on DMM. In this survey we will outline the results of Karp {\em et al.} in
 \cite{Karp} that significantly improved the state of art in this context
 employing the same two-choice balancing scheme we have seen in the
 previous section.

 \revadd{ Non ho ancora parlato dello state of art prima di Karp
   e neppure ho ancora dato i numeri}
 
\subsection{Models Definition}
  A PRAM is constituted of $P_1, \dots P_m$ processors
  and a shared memory with cells $U[1, \dots p]$. The processors
  works in parallel and have random access to the shared memory.
  Each memory cell of $U$ can store an integer and can be accessed
  by one processor at a time\footnote{This hypotheses in literature is referred
    to as ``exclusive read exclusive write'', and the machine we have defined
    is called EREW-PRAM. However we will write PRAM for brevity.}. \\
 \\
  A DMM has processors $P_1, \dots P_n$ communicating via distributed
  memory modules $M_1, \dots M_n$. Every module behaves like a PRAM
  shared memory. Processors works in parallel and each
  processor can access any module\footnote{In the general definition processors
    are connected with modules through an interconnection network. Since we are
    interested in measuring the delay induced by memory contention we will
    isolate its effect assuming a fully connected network.}.
  Read and Write operations on a module are handled by a module's window
  allowing concurrent read and concurrent write\footnote{They
    behave like CRCW-PRAM cells.}. The semantic of concurrent R/W operations
  is simple:
  when multiple processor tries to access simultaneously then one is chosen
  arbitrarily and can access the module, the others are returned a
  ``failure'' message. 

\subsection{Simulation}
Simulating a $n$-PRAM on a $n$-DMM means to write a program for a DMM having
$n$ processors that can execute programs written for a PRAM having
$n$ processors. We define the delay\footnote{Being eventually
  a random variable.} as the number of DMM operations necessary to simulate
a PRAM operation. In 1984 Mehlhorn {\em et al.} \cite{art} proved that PRAM
can be simulated on DMM with a delay of $O\bigl(\log\bigl(n\bigr)\bigr / \log\bigl(\log(n)\bigr))$
with high probability, they employed a single hashing scheme to
distribute virtual PRAM's
 memory cells among DMM's modules. 
We are now exposing an algorithm published by Karp {\em et al.} \cite{Karp} in
1996 that
outperforms the previous one simulating a PRAM with delay
$O\bigl(\log\bigl(\log(n)\bigr)\bigr)$ with high probabilty.
We are making some simplifications with respect to the original article,
the major one pertains the choice of hash functions.
In the original paper several families of hash
functions are carefully
devised so that they have good independence properties,
sufficient to prove the simulation 
performance. We will instead assume that hash functions are sampled from
a perfectly random family, as we did in the previous section. All the proofs
have been modified accordingly.

\subsection{Distributed Hash Tables}
In the algorithm that we want to present a distributed hash
table is employed to handle write operations efficiently.
Bast {\em et al.} \cite{Bast}
give randomized algorithms for realizing an hash table on an $n$-processor PRAM
performing the following operations:
\begin{itemize}
\item \textsc{Build}$\bigl(X\bigr)$ building the set from $n$ key-value
  pairs in $O\bigl(\log^*(n)\bigr)$\footnote{Where $\log^*(n)$ is the number of
    times the logarithm have to iteratively applied to $n$ to obtain a number
    less or equal than one (i.e. iterated logarithm).}
\item \textsc{Hash}$\bigl(\text{DHT, }X\bigr)$ inserting $n$ key-value pairs in  
  $O\bigl(\log^*(n)\bigr)$
\item \textsc{LookUp}$\bigl(\text{DHT, }Y\bigr)$ returning an array of $n$ key-value pairs in $O(1)$
\end{itemize}
where $X = \bigl\{S_1, \dots S_n\gibr\}$ is a set of $n$ key-value pairs,
$Y = \bigl\{K_1, \dots K_n\gibr\}$ is a set of $n$ keys, and DHT is the
distributed hash table performing the operation.
The semantic of the operation \textsc{LookUp} in case that a key is not present is that
the second value in the returned pair is ``failure''. For our purposes we want
the hash table not to occupy more than $O(n)$ PRAM's memory cells, it is
then sufficient to slightly modify the implementation of \textsc{Build} and
\textsc{Hash}
to make them return ``failure'' in case the structure would exceed that amount.
Finally all the operation complexities are intended with high probability.

So far we have an implementation of the hash table for a PRAM while we
want the data structure implemented on a DMM, the following lemma will
guarantee that we can implement the same structure with constant delay
as long as it does not
exceed $O(n)$ memory usage.

\begin{lemma}
  An $n$-processor PRAM with $m$ memory cells can be simulated on an
  $n$-processor DMM with delay $O\bigl(m / n\bigr)$.
\end{lemma}
\begin{proof}
  Partition the $m$ cells of PRAM into the $n$ modules of DMM. Now any parallel
  access to those cells can be simulated serializing it $O\bigl(m / n\bigr)$
  DMM operations.
\end{proof}



\subsection{The Algorithm}
As stated at the beginning of this section the cause of the delay
while simulating a DMM on a PRAM is
the memory contention. Then we should find a good scheme to evenly distribute
PRAM's virtual cells among DMM's modules. Mehlhorn {\em et al.} \cite{art} assignment
consisted in choosing one random function $h$ and assign cell $z$ to the
module $M_{h(z)}$. We will instead choose two hash functions $h_1$ and $h_2$
form the perfectly random function $\mathcal{H}$ and keep a copy of the cell
$z$ in both modules $M_{h_1(z)}$ and $M_{h_2(z)}$. In this way we will
end up reading the content of $z$ from the
least loaded\footnote{In this context ``loaded'' means queried by processors
requesti} module between $h_1$ and $h_2$ as we did
for assigning jobs in the previous section. Now we are exposing the
algorithms employed to simulate PRAM's read and write operations.
Let DHT$_1$ and DHT$_2$ denote two distributed hash tables, $X$ and $Y$ denote
sets of location-value pairs, $Z$ denotes a set of memory locations
and $C$ denote some constant.

\algblock{ParFor}{EndParFor}
\algnewcommand\algorithmicparfor{\textbf{parallel for}}
\algnewcommand\algorithmicpardo{\textbf{do}}
\algrenewtext{ParFor}[1]{\algorithmicparfor\ #1\ \algorithmicpardo}
\algrenewtext{EndParFor}{\algorithmicendparfor}

\begin{algorithm}[H]
  \begin{algorithmic}[1]
    \Procedure{Write}{$X$} \Comment{Write values of $X$ in PRAM's virtual cells}
      \For {$ j = 1,\; 2 $}
      \If {\Call{Hash}{DHT$_j$, $X$} = failure} \Comment{Try to store them
        in the table} 
      \State {\Call{Move}{DHT$_j \cup X$, $h_j$, $\infty$}} \Comment{Flush
        everything in case of failure}
        \Else
        \State {\Call{Move}{DHT$_j$, $h_j$, $C$}} \Comment{Flush a constant
          amount of values}
      \EndIf
     \EndFor
     \EndProcedure
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
  \begin{algorithmic}[1]     
    \Procedure{Move}{$Y$, $h$, $D$} \Comment{Try $D$ times to store
      $C$ values of $Y$ per module}
      \ParFor {$ i \in \left[1,\dots n\right]$} 
      \State {\Call{LocalRead}{$Y$, $C$, $M_i$} \Comment{Reads up to $C$ elements
          of $Y$ saved on $M_i$} 
      \For {$j \in \left[1,\dots C\right]$}
      \For {$D$ times, until succeeded}
      \State {\Call{Store}{$y_j$, $M_{h(y_j)}$} \Comment{DMM
          storing mechanism}
      \EndFor
      \EndFor
      \EndParFor
    \EndProcedure
  \end{algorithmic}
\end{algorithm}


\begin{algorithm}[H]
  \begin{algorithmic}[1]
    \Procedure{Read}{$Z$} \Comment {Read virtual cells in
      $Z = \left\{z_1,\dots z_n\right\}$}
    \For {$i \in \left[1, \dots n\right] $}
      \State {Status$[i]$ = failure}
     \EndFor
      \For {$ j = 1,\; 2$} \Comment{Read virtual cells stored
          in the table}
      \State {\Call{Lookup}{DHT$_j$, $Z$} 
      \ParFor {$i \in \left[1,\dots n\right]$}
      \If {$z_i$ found in DHT$_j$} 
        \State {Status$[i]$ = success}
        \EndIf
      \EndParFor
      \EndFor
      \While {$\exists \; k \in [1,\dots n]$ s.t. Status$[k]$ = failure}
      \label{while} \Comment{Until everything is read}
      \For {$j = 1,\; 2$} \Comment{Loads from the less busy module}
      \ParFor {$i \in \left[1,\dots n\right]$}
      \If {Status$[i]$ = failure}
      \State {outc = \Call{Load}{$z_i$, $M_{h_j(z_i)}$}}
      \Comment{DMM loading mechanism}
      \EndIf
      \If {Status$[i]$ = failure \&  outc = success} 
      \State {Status$[i]$ = success} \label{endwhile}
      
      \EndWhile
     \EndProcedure
  \end{algorithmic}
\end{algorithm}

The functions \textsc{Write} and \textsc{Read} simulate the homonym operations
for a PRAM. Let us now explain how the algorithm works. In order to guarantee
a low delay while reading cells we want to exploit the two choices scheme,
so we keep the content of cell $z$ in two different modules $M_{h_1(z)}$ and
$M_{h_2(z)}$. In this way we will end up retrieving $z$ in the least loaded
of the two, this implicitly happens in the cycle of lines
\ref{while}--\ref{endwhile} since
the DMM native method \textsc{Load} fulfill exactly one of the parallel requests at
a time returning ``failure'' to all the others.
On the other hand we ought to keep up to date both copies of $z$
every time we write into it. If we let the \textsc{Write} method update
both copies directly we obtain the $O\bigl(\log(n)\bigr / \log\bigl(\log(n)\bigr)\big)$ complexity achieved using only
one random chosen hash function \cite{art}. Then we need to employ two
distributed hash tables DHT$_1$ and DHT$_2$ to delay some writing operations.
As stated in the previous section these structures can be implemented keeping
the delay constant.









%%% Local Variables:
%%% mode: latex
%%% TeX-master: "paper"
%%% End:
