\section{PRAM Simulation}

Parallel machines that communicate via shared memory
(i.e. PRAM, Parallel Random Access Machine) are the most
commonly used computational model for describing parallel algorithms.
They provide a full abstraction over the memory management assuming
that each processor can access concurrently a shared memory. While being
very comfortable to program those machines are unrealistic from a technological
viewpoint, in fact parallel shared memory access can only be realized at the
cost of a significant delay. A more realistic model that takes distributed
memory into account is constituted by distributed memory machines (DMM) in which
the memory is divided into modules, one per processor. Since DMM's modules can
handle only one request at a time, therefore it emerges a phenomenon
called \textit{memory contention}.
In trying to get a deeper understanding of the performance
effects of memory contention it is useful to investigate the simulation of
 PRAM on DMM. In this survey we will outline the results of Karp {\em et al.} in
 \cite{Karp} that significantly improved the state of art in this context
 employing the same two-choice balancing scheme we have seen in the
 previous section.

 \revadd{ Non ho ancora parlato dello state of art prima di Karp
   e neppure ho ancora dato i numeri}
 
\subsection{Models Definition}
  A PRAM is constituted of $P_1, \dots P_m$ processors
  and a shared memory with cells $U[1, \dots p]$. The processors
  works in parallel and have random access to the shared memory.
  Each memory cell of $U$ can store an integer and can be accessed
  by one processor at a time\footnote{This hypotheses in literature is referred
    to as ``exclusive read exclusive write'', and the machine we have defined
    is called EREW-PRAM. However we will write PRAM for brevity.}. \\
 \\
  A DMM has processors $P_1, \dots P_n$ communicating via distributed
  memory modules $M_1, \dots M_n$. Every module behaves like a PRAM
  shared memory. Processors works in parallel and each
  processor can access any module\footnote{In the general definition processors
    are connected with modules through an interconnection network. Since we are
    interested in measuring the delay induced by memory contention we will
    isolate its effect assuming a fully connected network.}.
  Read and Write operations on a module are handled by the module's window
  allowing concurrent read and concurrent write\footnote{They
    behave like CRCW-PRAM cells.}. The semantic of concurrent R/W operations
  is simple:
  when multiple processor tries to access simultaneously then one is chosen
  arbitrarily and can access the module, the others are returned a
  ``failure'' message. 

\subsection{Simulation}
Simulating a $n$-PRAM on a $n$-DMM means to write a program for a DMM having
$n$ processors that can execute programs written for a PRAM having
$n$ processors. We define the delay\footnote{Being eventually
  a random variable.} as the number of DMM operations necessary to simulate
a PRAM operation. In 1984 Mehlhorn {\em et al.} \cite{art} proved that PRAM
can be simulated on DMM with a delay of $O\bigl(\log\bigl(n\bigr)\bigr)$
 with high probability employing a single hashing scheme. 
We are now exposing an algorithm published by Karp {\em et al.} \cite{Karp} in
1996 that
outperforms the previous one simulating a PRAM with delay
$O\bigl(\log\bigl(\log(n)\bigr)\bigr)$ with high probabilty.
Since the aim of this survey is to show an application of the power of the two
choices we are making some simplifications with respect to the original article.
In the original paper several families of hash
functions are carefully
devised so that they have good independence properties,
sufficient to prove the simulation 
performance. We will instead assume that hash functions are sampled from
a perfectly random family, as we did in the previous section. All the proofs
have been modified accordingly.

\subsection{Distributed Hash Tables}
In the algorithm that we want to present a distributed hash
table is employed to handle write operations efficiently.
Bast {\em et al.} \cite{Bast}
give randomized algorithms for realizing an hash table on an $n$-processor PRAM
performing the following operations:
\begin{itemize}
\item BUILD$\bigl(X\bigr)$ building the set from $n$ key-value
  pairs in $O\bigl(\log^*(n)\bigr)$\footnote{Where $\log^*(n)$ is the number of
    times the logarithm have to iteratively applied to $n$ to obtain a number
    less or equal than one (i.e. iterated logarithm).}
\item HASH$\bigl(\text{DHT, }X\bigr)$ inserting $n$ key-value pairs in  
  $O\bigl(\log^*(n)\bigr)$
\item LOOKUP$\bigl(\text{DHT, }Y\bigr)$ returning an array of $n$ key-value pairs in $O(1)$
\end{itemize}
where $X = \bigl\{S_1, \dots S_n\gibr\}$ is a set of $n$ key-value pairs,
$Y = \bigl\{K_1, \dots K_n\gibr\}$ is a set of $n$ keys, and DHT is the
distributed hash table performing the operation.
The semantic of the operation LOOKUP in case that a key is not present is that
the second value in the pair would be ``failure''. For our purposes we want
the hash table not to occupy more than $O(n)$ PRAM's memory cells, it is
then sufficient to slightly modify the implementation of BUILD and LOOKUP
to make them return ``failure'' in case the structure would exceed that amount.
Finally all the operation complexities are intended with high probability.

So far we have an implementation of the hash table for a PRAM while we
want the data structure implemented on a DMM, the following lemma will
guarantee that we can implement the same structure as long as it does not
exceed $O(n)$ memory usage.

\begin{lemma}
  An $n$-processor PRAM with $m$ memory cells can be simulated on an
  $n$-processor DMM with delay $O\bigl(m / n\bigr)$.
\end{lemma}
\begin{proof}
  Partition the $m$ cells of PRAM into the $n$ modules of DMM. Now any parallel
  access to those cells can be simulated serializing it $O\bigl(m / n\bigr)$
  DMM operations.
\end{proof}



\subsection{The Algorithm}
As stated at the beginning of this section the cause of the delay
while simulating a DMM on a PRAM is
the memory contention. Then we should find a good scheme to evenly distribute
PRAM's cells among DMM's modules. Mehlhorn {\em et al.} \cite{art} algorithm
consisted in choosing one random function $h$ and assigning cell $i$ to the
module $h(i)$. We will instead choose two hash function $h_1$ and $h_2$
form the perfectly random function $\mathcal{H}$ and assign sequentially
the cells to the least loaded module between $h_1$ and $h_2$ as we did
for assigning jobs in the previous section. Now we are exposing the
algorithms employed to simulate READ and WRITE operations in PRAM.

Let DHT$_1$ and DHT$_2$ denote two distributed hash tables.

\begin{algorithm}[H]
  \caption{}\label{pseudocode}
  \begin{algorithmic}[1]
    \Procedure{WRITE}{$X$}
      \For {($ j = 1, 2 $)} \label{pseudocode_outer}
      \If {HASH$\left(\text{DHT}_j, \; X\right)$ == failure}
      \State {WRITE_M$\left(\text{DHT}_j \cup X, h_j, \infty$}
        \Else
        \State {WRITE_M$\left(\text{DHT}_j \cup X, h_j, 4$}
      \EndIf
     \EndFor
    \EndProcedure
   \Procedure{WRITE_M}{$X,\; h,\; D$}
      \For {($ j = 1, 2 $)} \label{pseudocode_outer}
      \If {HASH$\left(\text{DHT}_j, \; X\right)$ == failure}
      \State {WRITE_M$\left(\text{DHT}_j \cup X, h_j, \infty$}
        \Else
        \State {WRITE_M$\left(\text{DHT}_j \cup X, h_j, 4$}
      \EndIf
      \EndFor
    \EndProcedure
  \   
  \end{algorithmic}
\end{algorithm}








%%% Local Variables:
%%% mode: latex
%%% TeX-master: "paper"
%%% End:
