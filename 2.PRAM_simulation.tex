\section{PRAM Simulation}

Parallel machines that communicate via shared memory
(i.e. PRAM, Parallel Random Access Machine) are the most
commonly used computational model for describing parallel algorithms.
They provide a full abstraction over the memory management assuming
that each processor can access concurrently a shared memory. While being
very comfortable to program those machines are unrealistic from a technological
viewpoint, in fact parallel shared memory access can only be realized at the
cost of a significant delay. A more realistic model that takes distributed
memory into account is constituted by distributed memory machines (DMM) in which
the memory is divided into modules, one per processor. Since DMM's modules can
handle only one request at a time, it emerges a phenomenon
called \textit{memory contention}.
In trying to get a deeper understanding of the performance
effects of memory contention it is useful to investigate the simulation of
 PRAM on DMM. In this survey we will outline the results of Karp {\em et al.} in
 \cite{Karp} that significantly improved the state of art in this context
 employing the same two-choice balancing scheme we have seen in the
 previous section.

\subsection{Models Definition}
  A PRAM is constituted of $P_1, \dots P_m$ processors
  and a shared memory with cells $U[1, \dots p]$. The processors
  works in parallel and have random access to the shared memory.
  Each memory cell of $U$ can store an integer and can be accessed
  by more than one processor simultaneously\footnote{This
    hypotheses in literature is referred
    to as ``concurrent read concurrent write'', and the machine we have defined
    is called CRCW-PRAM. However we will write PRAM for brevity.}. \\
 \\
  A DMM has processors $P_1, \dots P_n$ communicating via distributed
  memory modules $M_1, \dots M_n$. Every module behaves like a PRAM
  shared memory. Processors works in parallel and each
  processor can access any module\footnote{In the general definition processors
    are connected with modules through an interconnection network. Since we are
    interested in measuring the delay induced by memory contention we will
    isolate its effect assuming a fully connected network.}.
  Read and Write operations on a module are handled by a module's window
  allowing concurrent read and concurrent write\footnote{They
    behave like CRCW-PRAM cells.}. The semantic of concurrent read/write
  operations
  is simple:
  when multiple processors try to access a single module simultaneously then
  one processor is chosen
  arbitrarily and can access the module, all the others are returned a
  ``failure'' message. 

\subsection{Simulation}
Simulating a $n$-PRAM on a $n$-DMM means to write a program for a DMM having
$n$ processors that can execute programs written for a PRAM having
$n$ processors. We define the delay\footnote{Being eventually
  a random variable.} as the number of DMM operations necessary to simulate
a PRAM operation. In 1984 Mehlhorn {\em et al.} \cite{art} proved that PRAM
can be simulated on DMM with a delay of $O\bigl(\log\bigl(n\bigr)\bigr / \log\bigl(\log(n)\bigr))$
with high probability, they employed a single hashing scheme to
distribute virtual PRAM's
 memory cells among DMM's modules. 
We are now exposing an algorithm published by Karp {\em et al.} \cite{Karp} in
1996 that
outperforms the previous one simulating a PRAM with delay
$O\bigl(\log\bigl(\log(n)\bigr)\bigr)$ with high probability.
We are making some simplifications with respect to the original article,
the major one pertains the choice of hash functions.
In the original paper several families of hash
functions are carefully
devised so that they have good independence properties,
sufficient to prove the simulation 
performance. We will instead assume that hash functions are sampled from
a perfectly random family, as we did in the previous section. All the proofs
have been modified accordingly.

\subsection{Distributed Hash Tables}
In the algorithm that we want to present a distributed hash
table is employed to handle write operations efficiently.
Bast {\em et al.} \cite{Bast}
give randomized algorithms for realizing an hash table on an $n$-processor PRAM
performing the following operations:
\begin{itemize}
\item \textsc{Build}$\bigl(X\bigr)$ building the set from $n$ key-value
  pairs in $O\bigl(\log^*(n)\bigr)$\footnote{Where $\log^*(n)$ is the number of
    times the logarithm have to be iteratively applied to $n$ to obtain a number
    less or equal than one (i.e. iterated logarithm).}
\item \textsc{Hash}$\bigl(\text{DHT, }X\bigr)$ inserting $n$ key-value pairs in  
  $O\bigl(\log^*(n)\bigr)$
\item \textsc{Lookup}$\bigl(\text{DHT, }Y\bigr)$ returning an array of $n$ key-value pairs in $O(1)$
\end{itemize}
where $X = \bigl\{S_1, \dots S_n\gibr\}$ is a set of $n$ key-value pairs,
$Y = \bigl\{K_1, \dots K_n\gibr\}$ is a set of $n$ keys, and DHT is the
distributed hash table performing the operation.
The semantic of the operation \textsc{Lookup} in case that a key is not present is that
the second value in the returned pair is ``failure''. For our purposes we want
the hash table not to occupy more than $O(n)$ PRAM's memory cells, it is
then sufficient to slightly modify the implementation of \textsc{Build} and
\textsc{Hash}
to make them return ``failure'' in case the structure would exceed that amount.
Finally all the operation complexities are intended with high probability.

So far we have an implementation of the hash table for a PRAM while we
want the data structure implemented on a DMM, the following lemma will
guarantee that we can implement the same structure with constant delay
as long as it does not
exceed $O(n)$ memory usage.

\begin{lemma}\label{delay_lemma}
  An $n$-processor PRAM with $m$ memory cells can be simulated on an
  $n$-processor DMM with delay $O\bigl(m / n\bigr)$.
\end{lemma}
\begin{proof}
  Partition the $m$ cells of PRAM into the $n$ modules of DMM. Now any parallel
  access to those cells can be simulated serializing it $O\bigl(m / n\bigr)$
  DMM operations.
\end{proof}

\subsection{The Algorithm}
As stated at the beginning of this section the cause of the delay
while simulating a DMM on a PRAM is
the memory contention. Then we should find a good scheme to evenly distribute
PRAM's virtual cells among DMM's modules. Mehlhorn {\em et al.} \cite{art} assignment
consisted in choosing one random function $h$ and assign cell $z$ to the
module $M_{h(z)}$. We will instead choose two hash functions $h_1$ and $h_2$
form the perfectly random function $\mathcal{H}$ and keep a copy of the cell
$z$ in both modules $M_{h_1(z)}$ and $M_{h_2(z)}$. In this way we will
end up reading the content of $z$ from the
least queried module between $h_1$ and $h_2$ as we did
for assigning jobs in the previous section. Now we are exposing the
algorithms employed to simulate PRAM's read and write operations.
In the pseudocode DHT$_1$ and DHT$_2$ denote two distributed hash tables, $X$ and $Y$ denote
sets of location-value pairs, $Z$ denotes a set of memory locations
and $C$ denotes a constant.

\algblock{ParFor}{EndParFor}
\algnewcommand\algorithmicparfor{\textbf{parallel for}}
\algnewcommand\algorithmicpardo{\textbf{do}}
\algrenewtext{ParFor}[1]{\algorithmicparfor\ #1\ \algorithmicpardo}
\algrenewtext{EndParFor}{\algorithmicendparfor}

\begin{algorithm}[H]
  \begin{algorithmic}[1]
    \Procedure{Read}{$Z$} \Comment {Read virtual cells in
      $Z = \left\{z_1,\dots z_n\right\}$}
    \ParFor {$i \in \left[1, \dots n\right] $} \label{const}
      \State {Status$[i]$ = failure}
     \EndParFor
      \For {$ j = 1,\; 2$} \Comment{Read virtual cells stored
          in the table}
      \State {\Call{Lookup}{DHT$_j$, $Z$} 
      \ParFor {$i \in \left[1,\dots n\right]$}
      \If {$z_i$ found in DHT$_j$} 
        \State {Status$[i]$ = success} \label{endconst}
        \EndIf
      \EndParFor
      \EndFor
      \While {$\exists \; k \in [1,\dots n]$ s.t. Status$[k]$ = failure}
      \label{while} \Comment{Until everything is read}
      \For {$j = 1,\; 2$} \Comment{Loads from the less busy module}
      \ParFor {$i \in \left[1,\dots n\right]$}
      \If {Status$[i]$ = failure}
      \State {outc = \Call{Load}{$z_i$, $M_{h_j(z_i)}$}}
      \Comment{DMM loading mechanism}
      \EndIf
      \If {Status$[i]$ = failure \&  outc = success} 
      \State {Status$[i]$ = success} \label{endwhile}
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
  \begin{algorithmic}[1]
    \Procedure{Write}{$X$} \Comment{Write values of $X$ in PRAM's virtual cells}
      \For {$ j = 1,\; 2 $}
      \If {\Call{Hash}{DHT$_j$, $X$} = failure} \Comment{Hash table overflow} 
      \State {\Call{Move}{DHT$_j \cup X$, $h_j$, $\infty$}} \Comment{Flush
        everything in case of failure}
        \Else
        \State {\Call{Move}{DHT$_j$, $h_j$, $C$}} \Comment{Flush a constant
          amount of values}
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
  \begin{algorithmic}[1]     
    \Procedure{Move}{$Y$, $h$, $D$} \Comment{Try $D$ times to store
      $C$ values of $Y$ per module}
      \ParFor {$ i \in \left[1,\dots n\right]$} 
      \State {\Call{LocalRead}{$Y$, $C$, $M_i$} \Comment{Reads up to $C$ elements
          of $Y$ saved on $M_i$} 
      \For {$j \in \left[1,\dots C\right]$}
      \For {$D$ times, until succeeded}
      \State {\Call{Store}{$y_j$, $M_{h(y_j)}$} \Comment{DMM
          storing mechanism}
  \end{algorithmic}
\end{algorithm}
The functions \textsc{Write} and \textsc{Read} simulate the homonym PRAM's operations. Let us explain how the algorithm works.

\paragraph{{\bf Read}.} In order to guarantee
a low delay while reading cells we want to exploit the two choices scheme,
so we keep the content of cell $z$ in two different modules $M_{h_1(z)}$ and
$M_{h_2(z)}$ whenever it does not fit in the limited space distributed hash tables DHT$_1$ or DHT$_2$. As we stated above in this way we will end up
retrieving $z$ in the least queried
module of the two whenever we miss it in DHTs. This implicitly happens in the cycle of lines
\ref{while}--\ref{endwhile} since
the DMM native method \textsc{Load} fulfills exactly one of the parallel requests at
a time returning ``failure'' to all the others. To ensure the correctness and
of \textsc{Read} method we ought to ensure that the following invariant holds
at any time step of the simulation.
\paragraph{Invariant:} For each PRAM's virtual memory cell $z$ if the content of
$z$ is not stored in $DHT_1$ nor in $DHT_2$ then it is stored in both
$M_{h_1(z)}$ and $M_{h_2(z)}$.

\paragraph{{\bf Write}.}On the write side we need to keep the invariant true.
If we employed a naive algorithm and let \textsc{Write}
method update
both copies directly then we would obtain the $O\bigl(\log(n)\bigr / \log\bigl(\log(n)\bigr)\big)$ complexity achieved using only
one random chosen hash function \cite{art}. Thus we need to employ two
distributed hash tables DHT$_1$ and DHT$_2$ to delay some writing operations.
As stated in the previous section these structures can be implemented keeping
the delay constant.
The algorithm always try to write virtual cells on hash tables, if that causes
an hash table overflow then it performs write operations in the naive manner
and flushes the entire table content into the a priori assigned modules.
If the write operation on hash table succeeds then a constant amount of
DHTs content per processor is flushed into the proper modules.

\subsection{Analysis}
We will prove that this simulation causes a delay of at most $O\bigl(\log\bigl(\log(n)\bigl)\bigl)$ for both operations.

\paragraph{{\bf Read}.} It is sufficient to notice that lines
\ref{const}--\ref{endconst} takes constant
time thanks to the Lemma \ref{delay_lemma}. The number of steps necessary to
executes lines \ref{while}--\ref{endwhile} is the maximum number of queries
that a module actually handles. Considering the sequential scheme used
to process queries it turns out that we are reading each cell from the
least so far queried module, exactly like we did for jobs and machine in the previous section. Then from the Theorem \ref{bigthm} it follows that the
number of steps is $O\bigl(\log\bigl(\log(n)\bigl)\bigl)$ with high probability.

\paragraph{{\bf Write}.} While the correctness of this algorithm is easy to
grasp it is not as easy to prove a complexity upper bound. The analysis deals
with a complex stochastic process and its study is not within the focus of this
survey. The interested reader can find its analysis in Karp {\em et al.}
\cite{Karp} under the nickname of ``process\_2'' where it is also proven that
\textsc{Move} method takes $O\bigl(\log*(n)\bigr)$ time with high probability.
This leads to the conclusion that even \textsc{Write} method takes
$O\bigl(\log*(n)\bigr)$ time with high probability and the analysis is
completed.








%%% Local Variables:
%%% mode: latex
%%% TeX-master: "paper"
%%% End:
