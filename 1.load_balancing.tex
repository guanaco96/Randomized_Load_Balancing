\section{Load Balancing}

In a typical distributed server there are $m$ machines serving $n$ jobs
queried form clients. We want to assign jobs
to machines ensuring a fair load, a persistent choice, and preventing a
malicious client from unbalancing machines.
We define a job assignment ``fair'' if any machine gets approximately
$n / m$ jobs, it is of main importance that our assignment is fair since
it guarantees a predictable throughput.
If the same job is queried twice it is convenient to assign
it to the same machine since caching plays an important role in application
responsiveness, we call this property ``persistency''.  
The simplest assignment scheme is to assign the $i$-th job to the
$j$-th machine if $j \equiv i \pmod m$. This assignment is both fair and
persistent but presents a big downside: it is vulnerable to hacking. Immagine
that an adversarial client wants to slow down our server, it is sufficient
that he sends many queries belonging to the same class mod $m$. Then it is
paramount to employ more complex assignment schemes, in particular we will see
that randomized hashing will play a major role in this problem. Let us dig into
the mathematical tools necessary to deal with those random structures.

\subsection{Concentration Bounds}
Immagine a piece of
software that takes on average one second to load, but once in a while it
freezes for minutes. It is certainly not good software. Indeed one of the main
performance metric is the \textit{predictability}, that is the tendency
of execution
time not to deviate from expectation. As a consequence while devising
random algorithms we often do not content ourself of analyzing their
performance on average, instead we look for probabilistic guarantees
that execution times will concentrate around average.

For the rest of this subsection let $\bigl(X_i\bigr)$ be i.i.d. random
variables with mean $\mu$ and variance $\sigma^2$, and define $X = \sum_{i=1}^n X_i$ a random variable with mean $\bar{\mu} = n\mu$ and variance $\bar{\sigma}^2 = n\sigma^2$ . How is $X$ concentrated around $\mathbb{E}\bigl[X\bigr] = \bar{\mu}$?
Two well known concentration bounds are the Markov's inequality
\begin{equation*}
\pr\left(X \geq (1 + \delta)\bar{\mu}\right) \leq \frac{1}{1 + \delta}
\end{equation*}
and Chebyshev's inequality
\begin{equation*}
\pr\left(X \geq (1 + \delta)\bar{\mu}\right) \leq \frac{\bar{\sigma}^2}{ \bar{\mu}^2\delta^2} = \frac{\sigma}{n\mu^2\delta^2}.
\end{equation*}
The latter is more powerful and captures the law of large numbers. Now we expose
a more powerful concentration bound stating the exponential decay
of binomial distribution's tail.

\begin{theorem}{\textbf{(Chernoff Bound)}}
  If $\bigl(X_i\bigr)$ are i.i.d. Bernoulli random variables with $p = \pr\bigl(X_i = 1\bigl)$, $X$ is defined as above and $\marthbb{E}[X]=\bar{\mu}=np$ then
  \begin{equation}
    \pr\left(X \geq (1+\delta) \bar{\mu}\right) \leq \left(\frac{e^\delta}{(1+\delta)^{1+\delta}}\right)^{np}}
  \end{equation}\label{chernoff_1}
  and 
  \begin{equation}\label{chernoff_2}
    \pr\left(X \geq \bar{\mu} + \lambda\right) \leq e^{-\lambda^2 / (2\bar{\mu} + \lambda)}.
  \end{equation}
\end{theorem}
\begin{proof}
  Let $a \in \R$ and $t \geq 0$, apply Markov's inequality and
  factorize the expected value of independent variables
  \begin{equation*}
    \pr\left(X \geq a \right) = \pr\left( e^{tX} \geq e^{ta} \right) \leq
      e^{-ta} \cdot \mathbb{E} [e^{tX}] =
    e^{-ta} \cdot \prod_{i = 1}^n \mathbb{E}\left[e^{t X_i}\right]. 
  \end{equation*}
  Now bound $e^{X_i}$ from above
  \begin{equation*}
    \mathbb{E}\left[e^{tX_i}\right] = 1  + p \left( e^t - 1 \right) \leq
    e^{p \left( e^t - 1 \right)}.
  \end{equation*}
  Setting $t = \log(1 + \delta)$ and $a = (1+\delta)\bar{\mu} = (1+\delta)np$ we get
  \begin{equation*}
    \pr\left(X \geq (1+\delta)\bar{\mu} \right) \leq
     (1+\delta)^{-(1+\delta)np} \cdot \prod_{i = 1}^n e^{p\delta} =
    \left(\frac{e^\delta}{(1+\delta)^{1+\delta}}\right)^{np}.
  \end{equation*}
  To obtain the inequality \eqref{chernoff_2} from \eqref{chernoff_1} we
  operate the substitution $\lambda = \delta \bar{\mu}$ and prove the
  following inequality on RHS exponents
  \begin{equation*}
    \delta -(1+\delta)\log(1+\delta) \leq -\frac{\delta^2}{2 + \delta}  \end{equation*}
  being equivalent to
  \begin{equation*}
    f(\delta) := \frac{2\delta}{2+\delta} - \log(1+\delta) \leq 0.
  \end{equation*}
  Then it is sufficient to notice that $f(0) = 0$ and $f^\prime(\delta) \leq 0$ for all $\delta \geq 0$.
\end{proof}

\subsection{Randomized Hashing}
As already stated we want to prevent a single client from overloading
our machines unbalancing the job assignment. If our assignment scheme is
deterministic, then it is sufficient to invert it in order to send all the
queries to a single machine. We face this problem choosing the mapping
at random.

\begin{definition}{\textbf{(Perfectly Random Hashing)}
    Let $\left(\mathcal{H}, \pr \right)$ be a probability space where
    $\mathcal{H} \subset \left\{h \, | \, h : [1, \dots n]
      \rightarrow [1, \dots m]\right\} $ are hash functions.
    We define it a perfectly random hashing family if
    \begin{equation*}
      \pr\left(h(a) = \alpha_a \, \left| \, \bigwedge_{b \neq a}h(b) = \alpha_b
      \right\right) = \frac{1}{m} 
  \end{equation*}
  for each choice of $a$ and $\{\alpha_j\}_{j\leq n}$.
\end{definition}
Sampling $h$ form a perfectly random hashing family $\mathcal{H}$ and looking
at its images we get $\left\{h(i)\right\}_{i\leq n}$ i.i.d. uniform
random variables over $[1, \dots m]$.
In this survey we will always assume to dispose of perfectly random
hashing families. Unfortunately it is not possible to obtain $n$-wise
independence assuming that the hash functions have to be stored in
little space and evaluated quickly\footnote{Indeed Karp {\em et al.} in
\cite{Karp} carefully devise hash functions having approximate independence
properties.}. On the other hand this assumption allows us to prove tight
bounds on the load charged by assignment schemes based on these families.
Finally it is worth noting that, although in practice hash functions do not
produce $n$-wise independent values,the guarantees provided by theoretical
bounds are met. This suggests that we can trust bounds even if they are
proven exploiting stronger distributional hypotheses.

Now we dispose of every tool we need to present the first assignment scheme
being fair, persistent and preventing malicious attacks.
Recall that we have $n$ jobs to distribute over $m$ machines, we sample a
random hash function $h$ from the perfectly random family $\mathcal{H}$ and
assign the $i$-th job to $h(i)$. Let us analyze this algorithm.

\begin{theorem}{\textbf{(One Choice Balancing)}
    Given the assignment described above, and assuming $n / m$ constant,
    denote with $X_j$ the number of jobs assigned to the $j$-th machine, then
  \begin{equation*}
    \pr\left(\max_{j\leq m} X_j \leq \frac{n}{m} + 6 \log(n)\right) \geq
    1 - \frac{1}{n}.
  \end{equation*}
\end{theorem}

\begin{proof}
  Let $Y_j^i$ be a random variable evaluating one if $i$-th job is assigned
  to the $j$-th machine and zero otherwise. Thus $\bigl(Y_j^i\bigr)_{i\leq n}$
  are i.i.d. Bernoulli random variables and we get
  $X_j$ = $\sum_{i\leq n}Y_j^i$. Now apply the
  inequality \eqref{chernoff_2} with $\bar{\mu} = \mathbb{E}[X_j] = n / m$
  and $\lambda = 6 \log(n)$ to prove
  \begin{gather*}
    \pr\left(X_j > \frac{n}{m} + 6 \log(n)\right) \leq \\
    \exp\left(- \frac{\left(6\log(n)\right)^2}{2\frac{n}{m} +
    6\log(n)}\right) \leq\\
    \exp\left(-2\log(n) \right) = \frac{1}{n^2}
  \end{gather*}
  for each $j \leq m$. The proof is concluded via union bound.
\end{proof}

\subsection{Two Choices Load Balancing}
Although the previous assignment algorithm provided a good guarantee
in terms of maximum load on a single machine, we can do better keeping the
algorithm very simple. Now we sample two random hash function $h_1$ and $h_2$
from a perfectly random family $\mathcal{H}$, and process the jobs sequentially. We assign the $i$-th job to the least loaded machine so far
between $h_1(i)$ and $h_2(i)$.
For the sake of simplicity we will assume that $n=m$ and prove\footnote{This
  proof is rather technical and it has been taken from Mitzenmacher
  {\em et al.} \cite{Mitzenmacher}} the following theorem bounding
the maximum load.

\begin{theorem}{\textbf{(Two Choices Balancing)} \label{bigthm}
    Suppose that we assign $n$ jobs to $n$ machines following the algorithm
    above. Denoting with $X_j$ the number of jobs assigned to the $j$-th
    machine we get
    \begin{equation*}
      \pr\left(\max_{j\leq m} X_j \leq \log\left(\log(n)\right) + O(1)\right)
      \geq 1 - o\left(\frac{1}{n}\right).
    \end{equation*}
\end{theorem}
Before we can start the proof we need the following lemma.
\begin{lemma}\label{Binomial}
  Given $\bigl(X_i\bigr)_{i=1, \dots n}$ random variables, and for each
  $i\leq n$ a map $f_i$ taking values in $\{0, 1\}$ we define
  the boolean random variable $Y_i = f_i(X_1, \dots X_i)$.
  If for each $i \leq n$ and for every choice of $\alpha_1, \dots \alpha_n$
  \begin{equation*}
    \pr\left(Y_i = 1 \, \left| \, X_j = \alpha_j
    \; \forall j \leq i\right\right) \leq p
\end{equation*}
then
\begin{equation*}
  \pr\left(\sum_{i=1}^n Y_i > k\right) \leq
  \pr\left( B(n,p) > k\right)
\end{equation*}
where $B(n,p)$ is a Binomial random variable having parameters $n$ and $p$.
\end{lemma}
\begin{proof}{(Lemma)}
  It is sufficient to construct a probability space and random variables
  $\bigl(\tilde{Y_i}\bigr)_{i\leq n}$ and $\bigl(Z_i\bigr)_{i\leq n}$ such that
  $\bigl(Z_i\bigr)_{i\leq n}$  are i.i.d. Bernoulli, $Y_i$ has the same distribution of $\tilde{Y}_i$ for each $i\leq n$ and
  \begin{equation*}
    \pr\left(Y_i \leq Z_i\right) = 1.
  \end{equation*}
  The construction is rather technical and it does not concern this
  survey.
\end{proof}
\begin{proof}(Theorem)
  Let us start defining several variables:
  \begin{itemize}
  \item $\beta_i$ is a sequence such that $\beta_4 = n / 4$ and
    $\beta_{i+1} = 2\beta_i^2 / n$
  \item $h(t)$ is the number of jobs assigned to the machine chosen
    at time step t
  \item $v_i(t)$ is the number of machines having assigned at least $i$
    jobs at time step $t$
  \item $u_i(t)$ is the number of $s \leq t$ such that $h(s) \geq i$
  \item $E_i$ is the event $\left\{v_i(n) \leq \beta_i\left\}$.
  \end{itemize}
  Our first goal is to prove that there exists an $i^*=\log\left(\log(n)\right) + O(n)$ such that $\pr\left(E_{i^*}\right) \geq 1 + o(\frac{1}{n})$.     
  
  Now we consider $i$ fixed and define the boolean random variable
  $Y_t$ such that
  \begin{equation*}
    Y_t = 1 \iff h(t)\geq i+1 \; \land \; v_i(t-1) \leq \beta_i. 
  \end{equation*}
  If $X_s$ is a random variable denoting the machine chosen at time
  step $s$ then $Y_t$ is a deterministic function of $\bigl(X_s\bigr)_{s\leq t}$
  and by the definition of $Y_t$ it follows that
  \begin{equation*}
    \begin{gathered}
      \pr\left(Y_t = 1 \, \left| \, X_s = \alpha_s \; \forall s < t
        \right\right) \leq \\
      \pr\left(h(t) \geq i + 1 \, \left| \, X_s = \alpha_s \; \forall s < t
          \; \land \; v_i(t-1)\right\right) \leq
      \left(\frac{\beta_i}{n}\right)^2
    \end{gathered}
  \end{equation*}
  Since there are at most $\beta_i$ machines with $i$ jobs assigned at time
  $t-1$.
  Then defining $p_i = \bigl(\beta_i / n\bigr)^2$, and applying Lemma
  \eqref{Binomial} we get
  \begin{equation*}
    \pr\left(\sum_{t=1}^n Y_t > k \right) \leq \pr\left(B(n, p_i) > k\right).
  \end{equation*}
  Our next step will be to prove that
  \begin{equation}\label{eq1}
    \pr\left(\lnot E_{i+1}\, \left| \, E_i\right\right) =
    \pr\left(v_{i+1}(n) > \beta_{i+1}\, \left| \, E_i\right\right) \leq
    \frac{e^{-np_i/3}}{\pr\left(E_i\right)}.
  \end{equation}
  Indeed we have
  \begin{equation*}
    \begin{aligned}
      \pr\left(v_{i+1}(n) > \beta_{i+1}\, \left| \,E_i\right\right) \leq \\
      \pr\left(u_{i+1}(n) > \beta_{i+1}\, \left| \,E_i\right\right) = \\
      \pr\left(\sum_{t=1}^n Y_t > \beta_{i+1}\, \bigg| \,E_i\right) \leq \\
      \frac{\pr\left(\sum_{t=1}^n Y_t > \beta_{i+1}\right)}{\pr\left(E_i\right)}
      \leq \\
      \frac{\pr\left(B(n, p_i) > 2n p_i \right)}{\pr\left(E_i\right)} \leq \\ 
      \frac{e^{-np_i/3}}{\pr\left(E_i\right)}.
    \end{aligned}
  \end{equation*}
    The first inequality is valid since $v_i(t) \leq u_i(t)$ for each
    $i$ and $t$, the second equality is there since, conditioned to $E_i$,
    the sum    $\sum_{t=1}^nY_t$ counts the number of $t$ such that
    $h(t) \geq i$.
    Finally the last inequality is an application of Chernoff bound
    \eqref{chernoff_2}.
    If we restrict ourselves to values of $i$ such that $p_i n \geq 6\log(n)$
    we get
    \begin{equation*}
      \pr\left(\lnot E_{i+1} \, \left|\, E_i\right\right) =
      \pr\left(v_{i+1}(n) > \beta_{i+1} \, \left|\, E_i\right\right) \leq
      \frac{1}{n^2 \pr\left(E_i\right)}
    \end{equation*}
    and therefore
    \begin{equation}\label{eq2}
      \begin{aligned}
      \pr\left(\lnot E_{i+1}\right) =\\
      \pr\left(E_i\right) \cdot
      \pr\left(\lnot E_{i+1} \, \left|\, E_i\right\right) +
      \pr\left(\lnot E_i\right) \cdot
      \pr\left(\lnot E_{i+1} \, \left|\, \lnot E_i\right\right) \leq \\
      \pr\left(\lnot E_i\right) + \frac{1}{n^2}
    \end{aligned}
    \end{equation}
    that inducing over $i$ proves that for each $i$ s.t. $p_in \geq 6\log(n)$
    \begin{equation*}
      \pr\left(\lnot E_i\right) \leq \frac{i}{n^2}.
    \end{equation*}
    Consider the minimum $i^*$ such that $p_{i^*}n < 6\log(n)$,
    it is easy to prove by induction that
    \begin{equation*}
      p_{i+4} = 2^{-(2^{i+1} + 2)}
    \end{equation*}
    and hence $i^* = \log\left(\log(n)\right) + O(1)$. We have achieved
    our first goal proving that for each
    $i \leq \log\left(\log(n)\right) + O(1)$
    $\pr\left(\lnot E_i\right) \leq \frac{i}{n^2}$.

    Our final goal is to prove that
    $ \pr\left(v_{i^*+3} \geq 1 \right) = o\left(\frac{1}{n}\right)$.
    Meaning that there is no machine loaded with more than $i^* + 3$
    jobs with high probability. We need to prove the following two steps:
    \begin{enumerate}
    \item $\pr\left(v_{i^*+1}(n) > 12\log(n)\right)\leq (i^*+1) / n^2$
    \item $\pr\left(v_{i^*+3}(n) \geq 1)\leq \pr\left(u_{i^*+2}(n) \geq 2)$
    \end{enumerate}
    (1.) Noticing that
    $\beta_{i^* + 1} = 2 p_{i^*}n \leq 12 \log(n)$ and exploiting the
    inequality \eqref{eq1} we get
    \begin{equation*}
      \pr\left(v_{i^*+1}(n) > 12\log(n)\,\left|\, E_i\right \right)\leq
      \frac{1}{n^2\pr\left(E_i\right)}.
    \end{equation*}
    Applying the same argument of inequality \eqref{eq2} we obtain
    \begin{equation*}
      \pr\left(v_{i^*+1}(n) > 12\log(n)\right)\leq
      \pr\left(\lnot E_{i^*}\right) + \frac{1}{n^2} = \frac{i^*+1}{n^2}.
    \end{equation*}
    (2.) The variable $u_i(n)$ counts the number of jobs assigned to
    a machine loaded with at least $i$ jobs while the assignment took
    place. This entails that
    $$u_{i+1}(n) \geq 1 \implies u_i(n) \geq 2.$$
    Moreover from definitions follows easily that $v_i(n) \leq u_i(n)$ and this
    proves
    \begin{equation*}
      \pr\left(v_{i^*+3}(n) \geq 1)\leq 
      \pr\left(u_{i^*+3}(n) \geq 1\right)\leq
      \pr\left(u_{i^*+2}(n) \geq 2\right).
    \end{equation*}
    Finally we can conclude the proof
    \begin{equation*}
      \begin{gathered}
      \pr\left(v_{i^*+3}(n) \geq 1)\leq \\
        \pr\left(u_{i^*+2}(n) \geq 2\right) \leq \\
        \pr\left(v_{i^*+1}(n) > 12\log(n)\right) +
        \pr\left(v_{i^*+1}(n) \geq 12\log(n)\right) \cdot
        \pr\left(u_{i^*+2}(n) \geq 2 \,\left|\,v_{i^*+1}(n)
            \geq 12\log(n)\right \right) \leq \\
        \frac{i^*+1}{n^2} + \pr\left(B\left(n, \left(12\log(n) / n\right)^2
            \right) \geq 2 \right)
        \leq \\
        \frac{i^*+1}{n^2} + \binom{n}{2}
        \left(\frac{12\log(n)}{n}\right)^4 = o\left(\frac{1}{n}\right)
      \end{gathered}
    \end{equation*}
    where the last inequality is obtained via union bound
    over all the combinations.    
\end{proof}









%%% Local Variables:
%%% mode: latex
%%% TeX-master: "paper"
%%% End:
